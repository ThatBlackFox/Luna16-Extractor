{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install SimpleITK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_c, out_c, 3, padding=1)\n",
    "        self.norm1 = nn.InstanceNorm3d(out_c)\n",
    "        self.conv2 = nn.Conv3d(out_c, out_c, 3, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm3d(out_c)\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, 1),\n",
    "            nn.InstanceNorm3d(out_c)\n",
    "        ) if in_c != out_c else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = self.norm2(self.conv2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(VNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = ResidualBlock(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = ResidualBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.enc3 = ResidualBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResidualBlock(256, 512),\n",
    "            nn.Dropout3d(0.5)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up3 = nn.ConvTranspose3d(512, 256, 2, stride=2)\n",
    "        self.dec3 = ResidualBlock(512, 256)\n",
    "        self.up2 = nn.ConvTranspose3d(256, 128, 2, stride=2)\n",
    "        self.dec2 = ResidualBlock(256, 128)\n",
    "        self.up1 = nn.ConvTranspose3d(128, 64, 2, stride=2)\n",
    "        self.dec1 = ResidualBlock(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.final = nn.Conv3d(64, out_channels, 1)\n",
    "\n",
    "        # Attention Modules\n",
    "        self.attention3 = nn.Sequential(\n",
    "            nn.Conv3d(256, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention2 = nn.Sequential(\n",
    "            nn.Conv3d(128, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention1 = nn.Sequential(\n",
    "            nn.Conv3d(64, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )       \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "\n",
    "        # Decoder with Attention\n",
    "        d3 = self.up3(b)\n",
    "        if d3.size()[2:] != e3.size()[2:]:\n",
    "            d3 = F.interpolate(d3, size=e3.size()[2:], mode='trilinear', align_corners=False)\n",
    "        \n",
    "        att3 = self.attention3(e3)\n",
    "        e3 = e3 * att3\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        if d2.size()[2:] != e2.size()[2:]:\n",
    "            d2 = F.interpolate(d2, size=e2.size()[2:], mode='trilinear', align_corners=False)\n",
    "        \n",
    "        att2 = self.attention2(e2)\n",
    "        e2 = e2 * att2\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        if d1.size()[2:] != e1.size()[2:]:\n",
    "            d1 = F.interpolate(d1, size=e1.size()[2:], mode='trilinear', align_corners=False)\n",
    "        \n",
    "        att1 = self.attention1(e1)\n",
    "        e1 = e1 * att1\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        return self.final(d1)\n",
    "\n",
    "        # Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.108197895896446896160048741492_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.109002525524522225658609808059_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.109002525524522225658609808059_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.111172165674661221381920536987_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.124154461048929153767743874565_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.126264578931778258890371755354_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.128023902651233986592378348912_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.129055977637338639741695800950_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.130438550890816550994739120843_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.134996872583497382954024478441_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.134996872583497382954024478441_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.134996872583497382954024478441_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.134996872583497382954024478441_3.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.137763212752154081977261297097_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.137763212752154081977261297097_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.138080888843357047811238713686_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.139258777898746693365877042411_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.141069661700670042960678408762_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.154677396354641150280013275227_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.187451715205085403623595258748_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.187451715205085403623595258748_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.188209889686363159853715266493_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.188209889686363159853715266493_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.188376349804761988217597754952_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.188376349804761988217597754952_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.188376349804761988217597754952_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.188376349804761988217597754952_3.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.194440094986948071643661798326_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.202811684116768680758082619196_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.213140617640021803112060161074_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.213140617640021803112060161074_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.213140617640021803112060161074_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.216882370221919561230873289517_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.216882370221919561230873289517_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.219087313261026510628926082729_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.219087313261026510628926082729_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.219909753224298157409438012179_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.219909753224298157409438012179_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.227962600322799211676960828223_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.241570579760883349458693655367_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.249530219848512542668813996730_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.249530219848512542668813996730_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.249530219848512542668813996730_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.250863365157630276148828903732_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.272042302501586336192628818865_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.277445975068759205899107114231_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.277445975068759205899107114231_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.281489753704424911132261151767_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.293757615532132808762625441831_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.293757615532132808762625441831_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.294188507421106424248264912111_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.294188507421106424248264912111_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.294188507421106424248264912111_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.295298571102631191572192562523_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.295298571102631191572192562523_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.295420274214095686326263147663_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.295420274214095686326263147663_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.295420274214095686326263147663_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.295420274214095686326263147663_3.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.295420274214095686326263147663_4.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.303421828981831854739626597495_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.305858704835252413616501469037_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.305858704835252413616501469037_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.310548927038333190233889983845_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.313334055029671473836954456733_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.313605260055394498989743099991_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.313835996725364342034830119490_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.317087518531899043292346860596_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.323302986710576400812869264321_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.323859712968543712594665815359_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.323859712968543712594665815359_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.333145094436144085379032922488_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.334517907433161353885866806005_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.334517907433161353885866806005_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.334517907433161353885866806005_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.395623571499047043765181005112_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.404364125369979066736354549484_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.417815314896088956784723476543_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.430109407146633213496148200410_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.430109407146633213496148200410_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.430109407146633213496148200410_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.450501966058662668272378865145_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.450501966058662668272378865145_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.511347030803753100045216493273_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.511347030803753100045216493273_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.511347030803753100045216493273_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.525937963993475482158828421281_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.534006575256943390479252771547_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.534006575256943390479252771547_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.534006575256943390479252771547_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.534083630500464995109143618896_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.534083630500464995109143618896_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.534083630500464995109143618896_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.566816709786169715745131047975_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.621916089407825046337959219998_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.640729228179368154416184318668_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.716498695101447665580610403574_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.724251104254976962355686318345_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.724251104254976962355686318345_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.752756872840730509471096155114_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.805925269324902055566754756843_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.826812708000318290301835871780_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.826812708000318290301835871780_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.832260670372728970918746541371_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.832260670372728970918746541371_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.868211851413924881662621747734_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.868211851413924881662621747734_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.868211851413924881662621747734_2.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.898642529028521482602829374444_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.898642529028521482602829374444_1.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.905371958588660410240398317235_0.mhd\n",
      "Saved: output\\1.3.6.1.4.1.14519.5.2.1.6279.6001.979083010707182900091062408058_0.mhd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"best_model1.pth\"  # Updated model path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure VNet is imported or defined\n",
    "# Adjust the import if necessary\n",
    "\n",
    "# Initialize model\n",
    "model = VNet(in_channels=1, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Paths\n",
    "input_folder = \"D:\\segm\\ct_patch_dataset_for_training\"  # Folder containing .mha files\n",
    "output_folder = \"output\"  # Folder to save segmented .mha files\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to preprocess MHA files into tensors\n",
    "def preprocess_mha(mha_path):\n",
    "    image = sitk.ReadImage(mha_path)\n",
    "    array = sitk.GetArrayFromImage(image).astype(np.float32)\n",
    "\n",
    "    # Normalize to [0, 1] (Avoid division by zero)\n",
    "    epsilon = 1e-8\n",
    "    array = (array - array.min()) / (array.max() - array.min() + epsilon)\n",
    "\n",
    "    # Convert to tensor and add batch + channel dimension\n",
    "    tensor = torch.tensor(array).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, D, H, W)\n",
    "    return tensor.to(device), image\n",
    "\n",
    "# Function to post-process and save output as MHA\n",
    "def save_mha(output_tensor, reference_image, save_path):\n",
    "    output_array = output_tensor.squeeze(0).squeeze(0).cpu().numpy()  # Ensure correct shape\n",
    "\n",
    "    # Convert back to SimpleITK image\n",
    "    output_image = sitk.GetImageFromArray(output_array)\n",
    "    output_image.CopyInformation(reference_image)  # Maintain original metadata\n",
    "\n",
    "    # Save as .mha\n",
    "    sitk.WriteImage(output_image, save_path)\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "# Iterate through all MHA files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".mhd\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)  # Save with the same name\n",
    "\n",
    "        # Preprocess the input\n",
    "        input_tensor, ref_image = preprocess_mha(input_path)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            output = torch.sigmoid(output)  # Apply sigmoid for binary segmentation\n",
    "            output = (output > 0.5).float()  # Threshold to get binary mask\n",
    "\n",
    "        # Save output as MHA\n",
    "        save_mha(output, ref_image, output_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
